# Dataload Service Complete Setup

## ğŸ¯ **Overview**

The dataload-service has been completely configured with both FastAPI application and Celery workers, including comprehensive resource allocation, HPA (Horizontal Pod Autoscaler), and PDB (Pod Disruption Budget) configurations.

## ğŸ“Š **Service Architecture**

### **ğŸ—ï¸ Two-Tier Architecture**
1. **FastAPI Application**: REST API for data upload and management
2. **Celery Workers**: Background task processing for data loading

### **ğŸ“ File Structure**
```
application_deployment/dev/dataload-service/
â”œâ”€â”€ deployment.yml      # âœ… FastAPI + Celery deployments
â”œâ”€â”€ service.yml         # âœ… Service configuration
â”œâ”€â”€ configmap.yml       # âœ… Configuration
â”œâ”€â”€ secret.yml          # âœ… Secrets
â”œâ”€â”€ pv-pvc.yml          # âœ… Persistent storage
â”œâ”€â”€ hpa.yml             # âœ… HPA for both services
â””â”€â”€ pdb.yml             # âœ… PDB for both services
```

## ğŸ”§ **Resource Allocation**

### **ğŸš€ FastAPI Application**
```yaml
resources:
  requests:
    memory: "256Mi"
    cpu: "250m"
  limits:
    memory: "512Mi"
    cpu: "500m"
```

**Configuration Details:**
- **Category**: Low Usage Service
- **Memory**: 256Mi-512Mi (efficient for API operations)
- **CPU**: 250m-500m (moderate processing power)
- **Replicas**: 1-3 (HPA scaling)

### **âš¡ Celery Workers**
```yaml
resources:
  requests:
    memory: "512Mi"
    cpu: "500m"
  limits:
    memory: "1Gi"
    cpu: "1000m"
```

**Configuration Details:**
- **Category**: Medium Usage Service
- **Memory**: 512Mi-1Gi (higher for data processing)
- **CPU**: 500m-1000m (more processing power for tasks)
- **Replicas**: 2-6 (HPA scaling)

## ğŸ“ˆ **HPA Configuration**

### **ğŸ¯ FastAPI HPA**
```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: dataload-service-hpa
spec:
  scaleTargetRef:
    name: dataload-service
  minReplicas: 1
  maxReplicas: 3
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 70
```

**Scaling Behavior:**
- **CPU Threshold**: 70% utilization
- **Memory Threshold**: 70% utilization
- **Scale Range**: 1-3 replicas
- **Scale Up**: Aggressive (100% increase, 2 pods max per 15s)
- **Scale Down**: Conservative (10% decrease, 1 pod max per 60s)

### **âš¡ Celery HPA**
```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: dataload-service-celery-hpa
spec:
  scaleTargetRef:
    name: dataload-service-celery
  minReplicas: 2
  maxReplicas: 6
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 70
```

**Scaling Behavior:**
- **CPU Threshold**: 70% utilization
- **Memory Threshold**: 70% utilization
- **Scale Range**: 2-6 replicas
- **Scale Up**: Aggressive (100% increase, 2 pods max per 15s)
- **Scale Down**: Conservative (10% decrease, 1 pod max per 60s)

## ğŸ›¡ï¸ **PDB Configuration**

### **ğŸ¯ FastAPI PDB**
```yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: dataload-service-pdb
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app: dataload-service
      component: fastapi
```

### **âš¡ Celery PDB**
```yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: dataload-service-celery-pdb
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app: dataload-service
      component: celery
```

**High Availability:**
- **FastAPI**: At least 1 pod must remain available
- **Celery**: At least 1 pod must remain available
- **Zero Downtime**: Ensures service availability during maintenance

## ğŸ” **Health Probes**

### **ğŸš€ FastAPI Health Checks**
```yaml
readinessProbe:
  httpGet:
    path: /
    port: 8000
  initialDelaySeconds: 20
  periodSeconds: 25
  timeoutSeconds: 5
  failureThreshold: 3

livenessProbe:
  httpGet:
    path: /
    port: 8000
  initialDelaySeconds: 40
  periodSeconds: 30
  timeoutSeconds: 5
  failureThreshold: 3
```

### **âš¡ Celery Health Checks**
```yaml
readinessProbe:
  tcpSocket:
    port: 8000
  initialDelaySeconds: 30
  periodSeconds: 30
  timeoutSeconds: 5
  failureThreshold: 3

livenessProbe:
  tcpSocket:
    port: 8000
  initialDelaySeconds: 60
  periodSeconds: 30
  timeoutSeconds: 10
  failureThreshold: 3
```

## ğŸ’¾ **Persistent Storage**

### **ğŸ“ Storage Configuration**
```yaml
volumes:
  - name: uploads-storage
    persistentVolumeClaim:
      claimName: uploads-pvc

volumeMounts:
  - name: uploads-storage
    mountPath: /data/uploads
```

**Storage Details:**
- **Volume**: `uploads-pvc` (PersistentVolumeClaim)
- **Mount Path**: `/data/uploads`
- **Shared Access**: Both FastAPI and Celery share the same storage
- **Data Persistence**: Uploads survive pod restarts

## ğŸ”— **Service Integration**

### **ğŸŒ Service Discovery**
```yaml
env:
  - name: DISCOVERY_URL
    value: "http://192.168.1.211:8761"
  - name: SPRING_PROFILES_ACTIVE
    value: "dev"
  - name: CONFIG_URL
    value: "http://config-server"
```

### **âš¡ Celery Configuration**
```yaml
env:
  - name: CELERY_BROKER_URL
    value: "redis://192.168.1.217:6379/0"
  - name: CELERY_RESULT_BACKEND
    value: "redis://192.168.1.217:6379/0"
```

## ğŸš€ **Deployment Commands**

### **1. Deploy Dataload Service**
```bash
# Deploy the complete dataload service
kubectl apply -f application_deployment/dev/dataload-service/deployment.yml -n default
kubectl apply -f application_deployment/dev/dataload-service/service.yml -n default
kubectl apply -f application_deployment/dev/dataload-service/configmap.yml -n default
kubectl apply -f application_deployment/dev/dataload-service/secret.yml -n default
kubectl apply -f application_deployment/dev/dataload-service/pv-pvc.yml -n default
```

### **2. Deploy HPA and PDB**
```bash
# Deploy HPA configurations
kubectl apply -f application_deployment/dev/dataload-service/hpa.yml -n default

# Deploy PDB configurations
kubectl apply -f application_deployment/dev/dataload-service/pdb.yml -n default
```

### **3. Monitor Deployment**
```bash
# Check deployment status
kubectl get deployments -n default | grep dataload-service

# Check HPA status
kubectl get hpa -n default | grep dataload-service

# Check PDB status
kubectl get pdb -n default | grep dataload-service

# Check pods
kubectl get pods -n default | grep dataload-service
```

## ğŸ“Š **Monitoring Commands**

### **ğŸ” Service Monitoring**
```bash
# Check FastAPI service
kubectl describe deployment dataload-service -n default

# Check Celery workers
kubectl describe deployment dataload-service-celery -n default

# Check HPA scaling
kubectl describe hpa dataload-service-hpa -n default
kubectl describe hpa dataload-service-celery-hpa -n default

# Check resource usage
kubectl top pods -n default | grep dataload-service
```

### **ğŸ“ˆ Scaling Monitoring**
```bash
# Watch HPA scaling
kubectl get hpa dataload-service-hpa -n default -w
kubectl get hpa dataload-service-celery-hpa -n default -w

# Check scaling events
kubectl get events -n default --sort-by='.lastTimestamp' | grep dataload-service
```

## ğŸ¯ **Key Benefits**

### **1. Automatic Scaling**
- **FastAPI**: Scales 1-3 replicas based on 70% CPU/Memory utilization
- **Celery**: Scales 2-6 replicas based on 70% CPU/Memory utilization
- **Dual Metric**: Both CPU and memory are monitored
- **Smart Scaling**: Aggressive scale-up, conservative scale-down

### **2. High Availability**
- **Zero Downtime**: Both services remain available during maintenance
- **Load Distribution**: Pods spread across multiple nodes
- **Fault Tolerance**: Survives node failures and maintenance
- **Rolling Updates**: Supports rolling deployment strategies

### **3. Resource Optimization**
- **Efficient Allocation**: Appropriate resources for each service type
- **Cost Control**: Scale down during low usage
- **Performance**: Scale up during high demand
- **Stability**: Prevents resource exhaustion

### **4. Data Persistence**
- **Shared Storage**: Both services access the same upload directory
- **Data Safety**: Uploads survive pod restarts and scaling
- **Scalability**: Multiple workers can process the same data

## âœ… **Status: Complete**

### **What's Configured**
- âœ… **FastAPI Deployment**: Complete with resource allocation and health probes
- âœ… **Celery Deployment**: Complete with resource allocation and health probes
- âœ… **HPA Configuration**: Separate HPA for both services
- âœ… **PDB Configuration**: Separate PDB for both services
- âœ… **Persistent Storage**: Shared storage for uploads
- âœ… **Service Integration**: Eureka discovery and configuration
- âœ… **Health Probes**: Optimized for each service type
- âœ… **Resource Allocation**: Appropriate for service requirements

### **Resource Summary**
- **FastAPI**: 256Mi-512Mi memory, 250m-500m CPU, 1-3 replicas
- **Celery**: 512Mi-1Gi memory, 500m-1000m CPU, 2-6 replicas
- **Scaling**: 70% CPU/Memory utilization threshold
- **Availability**: At least 1 pod per service during maintenance

---

**ğŸ‰ Dataload service is completely configured and ready for production deployment!**

The service provides:
- **REST API** for data upload and management
- **Background processing** with Celery workers
- **Automatic scaling** based on resource utilization
- **High availability** with zero downtime
- **Persistent storage** for data safety
- **Comprehensive monitoring** and health checks 